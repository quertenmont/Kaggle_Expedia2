package lq

import java.lang._ 
import scala.reflect.runtime.universe.TypeTag
import scala.reflect.runtime.universe.TypeTag._
import scala.collection.mutable.ArrayBuffer
import scala.concurrent.duration._
import scala.concurrent._
import scala.concurrent.ExecutionContext.Implicits.global
import Numeric.Implicits._
import Ordering.Implicits._

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.expressions.Window
//import org.apache.spark.sql.hive.HiveContext

import org.apache.spark.ml.classification.{OneVsRest, LogisticRegression}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.clustering._
import org.apache.spark.mllib.random._
import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating


import org.apache.spark.ml._
import org.apache.spark.ml.classification._
import org.apache.spark.ml.feature._
import org.apache.spark.ml.evaluation._

import java.lang.Math
import java.text.SimpleDateFormat
import java.util.concurrent.TimeUnit
import java.util.{Calendar, Date}

import smile.classification._
import smile.data.{Attribute, NumericAttribute, NominalAttribute};

object ExpediaSQL {

   def getdateunit(s:String):Double = {
      if(s==""){
         scala.Double.NaN
      }else{
         val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
         var date = formatter.parse(s);
         val calendar = Calendar.getInstance();
         calendar.setTime(date);
         ((calendar.get(Calendar.DAY_OF_YEAR).toDouble / 365) + (calendar.get(Calendar.YEAR) - 2013).toDouble)/3.0;   //divide by 3 to be in [0-1] for date from 2013 till 2015
     }
   }


   def getyear(s:String):Double = {
      if(s==""){
         scala.Double.NaN
      }else{
         val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
         var date = formatter.parse(s);
         val calendar = Calendar.getInstance();
         calendar.setTime(date);
         (calendar.get(Calendar.YEAR) % 100).toDouble;
     }
   }

   def getmonth(s:String):Double = {
      if(s==""){
         scala.Double.NaN
      }else{
         val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
         var date = formatter.parse(s);
         val calendar = Calendar.getInstance();
         calendar.setTime(date);
         calendar.get(Calendar.MONTH ).toDouble;
      }
   }

   def getmonthunit(s:String):Double = {
      if(s==""){
         scala.Double.NaN
      }else{
         val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
         var date = formatter.parse(s);
         val calendar = Calendar.getInstance();
         calendar.setTime(date);
         calendar.get(Calendar.DAY_OF_YEAR).toDouble / 365;
      }
   }

   def getNDays(ci:String, co:String):Double = {
      if(ci=="" || co==""){
         scala.Double.NaN
      }else{
         val formatter = new SimpleDateFormat("yyyy-MM-dd");
         var dateCI = formatter.parse(ci);
         var dateCO = formatter.parse(co);
         return TimeUnit.DAYS.convert(dateCO.getTime()-dateCI.getTime(), TimeUnit.MILLISECONDS).toDouble
      }
   }

   def getmonthunit2(s:String):Double = {
      if(s==""){
         scala.Double.NaN
      }else{
         val formatter = new SimpleDateFormat("yyyy-MM-dd");
         var date = formatter.parse(s);
         val calendar = Calendar.getInstance();
         calendar.setTime(date);
         calendar.get(Calendar.DAY_OF_YEAR).toDouble / 365;
      }
   }

   def getfracioninweek(ci:String, co:String):Double = {
      if(ci=="" || co==""){
         scala.Double.NaN
      }else{
         val ndays = getNDays(ci, co).toInt
         val formatter = new SimpleDateFormat("yyyy-MM-dd");
         var date = formatter.parse(ci);
         val calendar = Calendar.getInstance();
         calendar.setTime(date);
         var ciDay = calendar.get(Calendar.DAY_OF_WEEK) -1  //0=Monday, Friday=4, 6=Sunday
         var NDaysInWeek=0
         for( i <- 0 to ndays ){
             if( ((ciDay+i)%7) < 4)NDaysInWeek+=1    //count friday has week end already
         }
         NDaysInWeek.toDouble / ndays
      }
   }


  case class user_location(country:Int , region:Int , city:Int)
  case class hotel_location(continent:Int , country:Int , market:Int);
  case class cico_date(ci:String , co:String)
  case class querry(id:Long, date_time:String , site_name:Int , posa_continent:Int , user:user_location, orig_destination_distance:scala.Double, user_id:Int , is_mobile:Int , is_package:Int , channel:Int , date:cico_date, srch_adults_cnt:Int , srch_children_cnt:Int , srch_rm_cnt:Int , srch_destination_id:Int , srch_destination_type_id:Int , is_booking:Int , cnt:Int , hotel:hotel_location, hotel_cluster:Int)
  case class result(t:Int, p:Seq[(Int,Double)])

  def printStat(results:org.apache.spark.rdd.RDD[(Long,result)]):Unit = {
    val statDestId =  results
       .map{ case(id,r) => (id, r.p.map{case (a,b) => a}) }
       .map(pair => Array[Boolean]( pair._2.size<=0,  pair._2.size==1,  pair._2.size==2,  pair._2.size==3,  pair._2.size==4,  pair._2.size==5, pair._2.size>5) )
       .map(r => r.map( b => if(b){1}else{0}) )
       .reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2) )

  }

  def Map5(EvaluatePerf:Boolean, label:String, results:org.apache.spark.rdd.RDD[(Long,result)], SampleSize:Option[Long]=None):Unit = {
       if(EvaluatePerf){
          Future {
          val out = results
          .map{ case(id,r) => Array[scala.Double](
             1, 
             r.p.map{case (a,b) => a}.distinct.take(5).view.zipWithIndex.map( p => if(p._1==r.t){1.toDouble/(p._2+1.0)}else{0.0} ).foldLeft(0.toDouble)(Math.max),
             if(r.p.size<=0){1}else{0},
             if(r.p.size==1){1}else{0},
             if(r.p.size==2){1}else{0},
             if(r.p.size==3){1}else{0},
             if(r.p.size==4){1}else{0},
             if(r.p.size==5){1}else{0},
             if(r.p.size> 5){1}else{0}
          )}
          .reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2) )

          if(SampleSize.isEmpty){
             println("Performance for %-15s with MAP@5 = %8f/%8f = %10.6f".format(label, out(1), out(0), out(1)/out(0)) )
          }else{
             println("Performance for %-15s with MAP@5 = %8f/%8f = %10.6f (%8f/%8f = %10.6f for filled entries)".format(label, out(1), SampleSize.get.toDouble, out(1)/SampleSize.get.toDouble, out(1), out(0), out(1)/out(0)) )
          }

          println("STAT:")
          println("   NEntries = " + out(0) )
          println("   with less than 5 Clusters = " + (out(0) - out(7) - out(8) ) )
          println("   with           0 Clusters = " + out(2) )
          println("   with           1 Clusters = " + out(3) )
          println("   with           2 Clusters = " + out(4) )
          println("   with           3 Clusters = " + out(5) )
          println("   with           4 Clusters = " + out(6) )
          println("   with           5 Clusters = " + out(7) )
          println("   with         > 5 Clusters = " + out(8) )
          Console.flush(); 

          }
       }
  }


  def getFeature(q:querry):Array[scala.Double] = { Array(
//     q.is_booking.toDouble,                                           //0.008960345628498184
     getfracioninweek(q.date_time, q.date.ci),                        //0.18652176538663412
     getdateunit(q.date_time).toDouble,                               //0.21222168148885226
     getmonthunit(q.date_time).toDouble,                              //0.12168157303697873
     getNDays(q.date_time, q.date.ci).toDouble / 500.0,               //0.11580736012338262
     getNDays(q.date.ci, q.date.co).toDouble / 10.0,                  //0.049839744582384465
//     q.is_mobile.toDouble,                                            //0.009771288917655701
//     q.is_package.toDouble,                                           //0.011969394406345533
       q.srch_adults_cnt.toDouble,
       q.srch_children_cnt.toDouble,
//     q.srch_adults_cnt.toDouble /q.srch_children_cnt.toDouble,        //0.013645113285712974
//     q.srch_rm_cnt.toDouble,                                          //0.00976872906573772
     //q.site_name.toDouble,                                         
//     q.posa_continent.toDouble,                                       //0.013339604126049788
     q.orig_destination_distance,                                     //0.20424295417203686
     math.min(q.channel.toDouble, 9.0)                                //0.04223044577973103
  )}

  def getFeatureAttributes():Array[Attribute] = { Array[Attribute](
//     new NumericAttribute("isBooking"),
     new NumericAttribute("fracWeek"),
     new NumericAttribute("date"),
     new NumericAttribute("month"),
     new NumericAttribute("bookDateDelta"),//     new NumericAttribute("ndays"),
     new NumericAttribute("ndays"),
//     new NumericAttribute("isMobile"),
//     new NumericAttribute("isPackage"),
     new NumericAttribute("nAdult"),
     new NumericAttribute("nChild"),
//     new NumericAttribute("fracKid"),
//     new NumericAttribute("nRoom"),
     //new NumericAttribute("siteName"),//, Array[String]("0", "1", "2", "3", "4")),
//     new NominalAttribute("continent", Array[String]("0", "1", "2", "3", "4")),
     new NumericAttribute("distance"),
     new NominalAttribute("channel", Array[String]("0", "1", "2", "3", "4", "5", "6", "7", "8", "9"))
  )}

  def groupAndSave(EvaluatePerf:Boolean, save:Boolean, outputDir:String, groups:Array[org.apache.spark.rdd.RDD[(Long,result)]]):Unit = {
     val results = groups.reduceLeft{ (A,B) => 
        A.cogroup(B).map{ case(id, g) => (id, result(g._1.head.t, (if(g._2.size>0){g._1.head.p++g._2.head.p.take(10)}else{g._1.head.p})  )) }
     } 

     Map5(EvaluatePerf, outputDir, results)

     if(save){
        results
        .map{ case(id,r) => (id, r.p.map{case (a,b) => a}.distinct.take(5)) }
        .map{ case(id,p) => id.toString + "," + p.mkString(" ") }
        .coalesce(10)
        .saveAsTextFile("file:///afs/cern.ch/user/q/querten/scratch0/Expedia/" + outputDir);
     }
  }

  def groupAndSaveReorder(EvaluatePerf:Boolean, save:Boolean, outputDir:String, groups:Array[org.apache.spark.rdd.RDD[(Long,result)]]):Unit = {
     val results = groups.reduceLeft{ (A,B) => 
        A.cogroup(B).map{ case(id, g) => (id, result(g._1.head.t, (if(g._2.size>0){g._1.head.p++g._2.head.p.take(5)}else{g._1.head.p})  )) }          
     }
    
     val results2 = results.mapValues{ r => result(r.t, r.p.groupBy(_._1).mapValues{ listKeyValuePairs => (listKeyValuePairs.map(_._2).reduce{_+_} ) }.toSeq.sortWith{ case(a,b) =>  a._2 > b._2 } ) }
     Map5(EvaluatePerf, outputDir, results2)

     if(save &&  !EvaluatePerf){
        results2
        .map{ case(id,r) => (id, r.p.map{case (a,b) => a}.distinct.take(5)) }
        .map{ case(id,p) => id.toString + "," + p.mkString(" ") }
        .coalesce(10)
        .saveAsTextFile("file:///afs/cern.ch/user/q/querten/scratch0/Expedia/" + outputDir);
     }
  }

  def preprocess(groups:Array[org.apache.spark.rdd.RDD[(Long,result)]]):Unit = {
     groups.foreach( rdd => rdd.countAsync() )
  }

  def append_0(q:querry):java.lang.Double = { (getyear(q.date_time) - 12) + getmonthunit(q.date_time) }  //CHANGE wrt camilo
  def append_1(q:querry):java.lang.Double = { math.pow(append_0(q),3) * (6 + 19.0* q.is_booking.toDouble) } 
  def append_2(q:querry):java.lang.Double = { (2 + (8.0 * q.is_booking.toDouble)) }


  def main(args: Array[String]) {
        val conf = new SparkConf()
        conf.setAppName("ExpediaSQL")
        conf.set("spark.driver.memory", "4g")
        conf.set("spark.executor.memory", "2g")
        conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") //to avoid serialization issues with smile
//        conf.set("spark.kryoserializer.buffer.mb", "256"); 
        conf.set("spark.kryoserializer.buffer.max", "256mb");
        val sc = new SparkContext(conf)
        sc.setLogLevel("WARN")

        val sqlContext = new org.apache.spark.sql.SQLContext(sc)
        import sqlContext.implicits._

        //OPTIONS
        val EvaluatePerf = false;
        val useMVA       = false;  //CHANGE wrt camilo
        val CamiloMPA5   = false

        var trainFile = "file:///afs/cern.ch/user/q/querten/workspace/public/SparkTest/Data/train.csv"
        var testFile  = "file:///afs/cern.ch/user/q/querten/scratch0/test.csv"

        if(CamiloMPA5){
            trainFile = "file:///afs/cern.ch/user/q/querten/workspace/public/SparkTest/check_score/train20132014I.csv"
            testFile  = "file:///afs/cern.ch/user/q/querten/workspace/public/SparkTest/check_score/test_2014II_isbooking.csv"
        }

 	//Loading the train data into RDD with split
	val inputTrainDataRowRDD =sc.textFile(trainFile, 500).filter(! _.contains("date_time") ).map(_.split(",")).zipWithUniqueId().map{ case(q,index) => 
           var distance:Double = {if(q(6).length>0){q(6).toDouble}else{-1}}// (q(6).toDouble*1E5).toInt}else{-1} }
           querry(-1*index, q(0),q(1).toInt,q(2).toInt, user_location(q(3).toInt,q(4).toInt,q(5).toInt),distance,q(7).toInt,q(8).toInt,q(9).toInt,q(10).toInt, cico_date(q(11),q(12)),
                  q(13).toInt, q(14).toInt, q(15).toInt, q(16).toInt, q(17).toInt, q(18).toInt, q(19).toInt, hotel_location(q(20).toInt, q(21).toInt, q(22).toInt), q(23).toInt )
        }

 	//Loading the test data into RDD with split
	val inputTestDataRowRDD =sc.textFile(testFile, 250).filter(! _.contains("date_time") ).map(_.split(",")).map{ q =>
           var distance:scala.Double = {if(q(7).length>0){q(7).toDouble}else{-1}}//{(q(7).toDouble*1E5).toInt}else{-1} }
           querry(q(0).toLong, q(1),q(2).toInt,q(3).toInt,user_location(q(4).toInt,q(5).toInt,q(6).toInt),distance,q(8).toInt,q(9).toInt,q(10).toInt,q(11).toInt,cico_date(q(12),q(13)), 
                  q(14).toInt, q(15).toInt, q(16).toInt, q(17).toInt, q(18).toInt, 1, 1, hotel_location(q(19).toInt, q(20).toInt, q(21).toInt), -1 )
        }

        
        //define the training and test samples that we are going to use
        val Array(trainDataRowRDD, testDataRowRDD) = {
           val toReturn = {
           if(EvaluatePerf && !CamiloMPA5){
              val training = inputTrainDataRowRDD.filter(q => getyear(q.date_time)<14 || getmonthunit(q.date_time)<0.5)
              val testing  = inputTrainDataRowRDD.filter(q => getyear(q.date_time)==14 && getmonthunit(q.date_time)>0.5 && q.is_booking==1 && q.cnt==1 )
              val trainingUsers = sc.broadcast( training.map(q => q.user_id).distinct.collect() )
              Array( training, testing.filter(q => trainingUsers.value.contains(q.user_id) ) )  //keep only testing corresponding to user in the training set
           }else if(EvaluatePerf){
              val TestTruth = sc.textFile("file:///afs/cern.ch/user/q/querten/workspace/public/SparkTest/check_score/int_hc_loic.txt").filter(! _.contains("hotel_cluster")).map(_.split(" ")).map{ IdandTruth => (IdandTruth(0).toLong, IdandTruth(1).toInt)}
              Array(
                 inputTrainDataRowRDD, 
                 inputTestDataRowRDD.map(q => (q.id.toLong, q)).cogroup(TestTruth).flatMap{case (key, values) => 
                    values._1.map(q => q.copy(hotel_cluster=values._2.head))  //add back the truth to the same data structure
                 }
              )
           }else{
              Array(inputTrainDataRowRDD, inputTestDataRowRDD )
           }
           }.map(_.cache())
           toReturn.map(rdd => Future { rdd.count()  }).foreach(f => Await.ready(f, 10000 minutes)) //run an action to trigger the processing of the samples in parallel
           toReturn
        }

        //Init : DEFINE THE OUTPUT FORMAT AND USE IT EVERYWHERE  (ALLOWS to comment some part of the code without hurts)
        val totalstartTime = System.nanoTime()
        val TEST = testDataRowRDD;
        val TESTsize = TEST.count

        //println("site_name = " + TEST.map(q => q.site_name.toDouble ).stats)


/////////////////////////////////////////////////////////////////////////////// 
///     INITIALIZATION 
///////////////////////////////////////////////////////////////////////////////  

        //get the most frequent clusters overall  (will be use for sorting in case of equality to avoid randomness in equality)
        val clusterFrequency =  trainDataRowRDD.map(q => ( q.hotel_cluster, append_0(q) ) ).reduceByKey(_+_).sortBy(_._2, false)  //Orered Sequence of (Cluster,Rate)
        val clusterFrequencyBD=  sc.broadcast( clusterFrequency.collect.toMap ) //broadcast the variable for performance optimization


/////////////////////////////////////////////////////////////////////////////// 
///     A:  MAKE A FIRST PREDICTION BASED ON THE DATA LEAK WITHOUT MVA
///////////////////////////////////////////////////////////////////////////////  
/*
        //Build a lookup table of Ordered (clusters, Rate) for every (UserCity, Distance)
        val groupedLeakClassifier = trainDataRowRDD
           .filter{q => (q.orig_destination_distance >=0) }
           .map{q => ( (q.user.city, q.orig_destination_distance) , q)  }
           .groupByKey()
           .mapValues{ querries => querries.map(q => (q.hotel_cluster, append_0(q) )).groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
           }

        //Parse the train data and check if they are matches
        val resultsA = TEST
        .filter(q => q.orig_destination_distance >=0 ) 
        .map(q => ( (q.user.city, q.orig_destination_distance), q)  )
        .cogroup(groupedLeakClassifier)
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, values) => val testQuerries = values._1; val leakClusters = values._2.head;
           testQuerries.map(q => (q.id, result(q.hotel_cluster, leakClusters.take(10))) ) 
        }.cache()
        //Map5(EvaluatePerf,  "UserLoc, Dist", resultsA, Option(TESTsize))

        //println("FRACTION OF results in A with exactly   one choice = %6.4f%%".format( (resultsA.filter(pair => pair._2.p.size==1).count / resultsA.count.toDouble)*100.0 ) )
        //println("FRACTION OF results in A with more than one choice = %6.4f%%".format( (resultsA.filter(pair => pair._2.p.size >1).count / resultsA.count.toDouble)*100.0 ) )
*/


/////////////////////////////////////////////////////////////////////////////// 
///     A:  MAKE A FIRST PREDICTION BASED ON THE DATA LEAK WITH MVA
///////////////////////////////////////////////////////////////////////////////  

        //Parse the train data and check if they are matches
        val resultsAMap = TEST
        .filter(q => q.orig_destination_distance >=0 ) 
        .map(q => ( (q.user.city, q.orig_destination_distance), q)  )
        .cogroup( trainDataRowRDD.filter{q => (q.orig_destination_distance >=0) }.map{q => ( (q.user.city, q.orig_destination_distance) , q)}    )
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
//           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).zipWithIndex.toMap
           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters


            if(hotelClustersOrdered.size>1 && train.count(q => true)>25){
              val attributes = getFeatureAttributes() 
              val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
              val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 250, 2*hotelClustersMap.size)
              test.map{q =>
                     val feature   = getFeature(q)
                     var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                     classifier.predict(feature , posterior)
//                   val prediction = posterior.toSeq.zipWithIndex.sortWith(_._1 > _._1).map(pair => hotelClustersOrdered(pair._2))
                   val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
        val resultsA = resultsAMap.map(a => (a._1, a._2) ).cache()




/////////////////////////////////////////////////////////////////////////////// 
/// FIXME TEST0b: TRYING TO MAP BASED ON DATALEAK AND RANDOM FOREST
///////////////////////////////////////////////////////////////////////////////  

        //Parse the train data and check if they are matches
        val resultsTEST0bMap = TEST
        .filter(q => q.orig_destination_distance >=0 ) 
        .map(q => ( (q.user.city, q.orig_destination_distance), q)  )
        .cogroup( trainDataRowRDD.filter{q => (q.orig_destination_distance >=0) }.map{q => ( (q.user.city, q.orig_destination_distance) , q)}    )
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) =>
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters


            if(hotelClustersOrdered.size>1 && train.count(q => true)>25){
              val attributes = getFeatureAttributes() 


           val hotelClustersMapB     = train.filter(q=> q.is_booking==1).map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters
           val hotelClustersMapBswap = hotelClustersMapB.toSeq.map( pair => (pair._2, pair._1)).toMap

           val hotelClustersMapC     = train.filter(q=> q.is_booking==0).map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters
           val hotelClustersMapCswap = hotelClustersMapC.toSeq.map( pair => (pair._2, pair._1)).toMap

              val labelsB:Array[scala.Int] = train.filter(q=> q.is_booking==1 && hotelClustersMapB.contains(q.hotel_cluster)).map(q=> hotelClustersMapB(q.hotel_cluster).toInt ).toArray
              val featuresB:Array[Array[scala.Double] ] = train.filter(q=> q.is_booking==1 && hotelClustersMapB.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifierB = if(hotelClustersMapB.size>=2){new AdaBoost(attributes,  featuresB, labelsB, 250, 2*hotelClustersMapB.size)}else{null}

              val labelsC:Array[scala.Int] = train.filter(q=> q.is_booking==0 && hotelClustersMapC.contains(q.hotel_cluster)).map(q=> hotelClustersMapC(q.hotel_cluster).toInt ).toArray
              val featuresC:Array[Array[scala.Double] ] = train.filter(q=> q.is_booking==0 && hotelClustersMapC.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifierC = if(hotelClustersMapC.size>=2){new AdaBoost(attributes,  featuresC, labelsC, 250, 2*hotelClustersMapC.size)}else{null}

              test.map{q =>
                     val feature   = getFeature(q)
                     var posteriorB = Array.fill[scala.Double]( hotelClustersMapB.size)(0.0)
                     if(classifierB!=null){classifierB.predict(feature , posteriorB)}else{posteriorB=posteriorB.map(a=>1.0)}

                     var posteriorC = Array.fill[scala.Double]( hotelClustersMapC.size)(0.0)
                     if(classifierC!=null){classifierC.predict(feature , posteriorC)}else{posteriorC=posteriorC.map(a=>1.0)}

                   val predictionB = posteriorB.toSeq.zipWithIndex.map{ case(p,index) => (hotelClustersMapBswap(index),0.75*p) }
                   val predictionC = posteriorC.toSeq.zipWithIndex.map{ case(p,index) => (hotelClustersMapCswap(index),0.25*p) }
                   val predictionBC  = predictionB.union(predictionC).groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }.toMap
                   val prediction = hotelClustersOrdered.sortWith( (a,b) => a._2*predictionBC.getOrElse(a._1,0.0) > b._2*predictionBC.getOrElse(b._1,0.0) ).toSeq.sortWith{ case(a,b) => a._2 > b._2}

                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)


//                   val prediction = posterior.toSeq.zipWithIndex.sortWith(_._1 > _._1).map(pair => hotelClustersOrdered(pair._2))
                   //val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
        val resultsTEST0b = resultsTEST0bMap.map(a => (a._1, a._2) ).cache()



/////////////////////////////////////////////////////////////////////////////// 
/// FIXME TEST1: TRYING TO MAP BASED ON DATALEAK AND USERID AT ONCE
///////////////////////////////////////////////////////////////////////////////  
/*
        //Build a lookup table of Ordered (clusters, Rate) for every (UserCity, Distance)
        val groupedLeakUser = trainDataRowRDD
           .filter{q => (q.orig_destination_distance >=0) }
           .map{q => ( (q.user_id, q.user.city, q.orig_destination_distance) , q)  }
           .groupByKey()
           .mapValues{ querries => querries.map(q => (q.hotel_cluster, append_0(q) )).groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
           }

        //Parse the train data and check if they are matches
        val resultsTEST1 = TEST
        .filter(q => q.orig_destination_distance >=0 ) 
        .map(q => ( (q.user_id, q.user.city, q.orig_destination_distance), q)  )
        .cogroup(groupedLeakUser)
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, values) => val testQuerries = values._1; val leakClusters = values._2.head;
           testQuerries.map(q => (q.id, result(q.hotel_cluster, leakClusters)) ) 
        }
        Map5(EvaluatePerf,  "UserId, UserLoc, Dist", resultsTEST1, Option(TESTsize) )

        println("FRACTION OF results in TEST1 with exactly   one choice = %6.4f%%".format( (resultsTEST1.filter(pair => pair._2.p.size==1).count / resultsTEST1.count.toDouble)*100.0 ) )
        println("FRACTION OF results in TEST1 with more than one choice = %6.4f%%".format( (resultsTEST1.filter(pair => pair._2.p.size >1).count / resultsTEST1.count.toDouble)*100.0 ) )


       val resultsTEST1Cleaned = resultsTEST1.cogroup(resultsA).mapValues{ case(g1,g2) => 
          if(g1.size>0){
             g1.head
          }else{
             g2.head
          }
       }
        Map5(EvaluatePerf,  "UserId, UserLoc, Dist", resultsTEST1Cleaned, Option(TESTsize) )
        println("FRACTION OF results in TEST1Cwith exactly   one choice = %6.4f%%".format( (resultsTEST1Cleaned.filter(pair => pair._2.p.size==1).count / resultsTEST1Cleaned.count.toDouble)*100.0 ) )
        println("FRACTION OF results in TEST1Cwith more than one choice = %6.4f%%".format( (resultsTEST1Cleaned.filter(pair => pair._2.p.size >1).count / resultsTEST1Cleaned.count.toDouble)*100.0 ) )


        //printout entries with >1 options
        val querryList = TEST
        .filter(q => q.orig_destination_distance >=0 ) 
        .filter(q => q.id < 1000)
        .map(q => ( q.id, q)  )
        .cogroup(resultsTEST1, resultsA.filter(pair => pair._2.p.size >1) )
        .filter(_._2._3.size>0)
        .take(1000).foreach{ case (key, values) =>
           println("-- DEBUG ID "+ key)
           values._3.foreach(a => println(" A --> " + a.toString  ) )
           values._2.foreach(a => println(" 1 --> " + a.toString  ) )
           values._1.foreach(a => println(" Q --> " + a.toString  ) )
        }
        Console.flush(); 
*/


/////////////////////////////////////////////////////////////////////////////// 
/// FIXME TEST2: TRYING TO MAP BASED ON DATALEAK AND USERID AT ONCE
///////////////////////////////////////////////////////////////////////////////  
/*
        val resultsTEST2 = TEST
        .filter(q => q.orig_destination_distance >=0 )
        .filter(q => q.id < 1000)
        .map(q => ( (q.user.city, q.orig_destination_distance), q)  )
        .cogroup(trainDataRowRDD.filter{q => (q.orig_destination_distance >=0) }.map{q => ( (q.user.city, q.orig_destination_distance) , q)  } )
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .take(1000)
        .foreach{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).zipWithIndex.toMap


           val hotelChannelOrdered = train.map{ q => (q.channel, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
           val hotelChannelMap     = hotelChannelOrdered.map((a=>a._1)).zipWithIndex.toMap



           if(hotelClustersOrdered.size>1){
              println("-- COGROUPDEBUG ID "+ key + " : Best Hotels --> " + hotelClustersOrdered.mkString("|"))
              train.foreach(a => println(" -- -- TRAIN: " + a) );

              val attributes = Array[Attribute]( new NumericAttribute("InWeek"), new NumericAttribute("Date"), new NumericAttribute("Month"),  new NumericAttribute("NDays1"),  new NumericAttribute("NDays2"),  new NumericAttribute("isMobile"),  new NumericAttribute("IsPackage") ) //,  new NominalAttribute("Channel") )  
              var labels   = ArrayBuffer[scala.Int]()
              var features = ArrayBuffer[Array[scala.Double] ]()
              train.toSeq.sortWith( (a,b) => getmonthunit(a.date_time) >  getmonthunit(b.date_time)   )foreach{ q =>
                 labels += hotelClustersMap(q.hotel_cluster)
                 features += Array[scala.Double](
                    getfracioninweek(q.date_time, q.date.ci),
                    getdateunit(q.date_time).toDouble, 
                    getmonthunit(q.date_time).toDouble, 
                    getNDays(q.date_time, q.date.ci).toDouble / 500.0,
                    getNDays(q.date.ci, q.date.co).toDouble / 10.0,
                    q.is_mobile.toDouble,
                    q.is_package.toDouble//,
//                    hotelChannelMap(q.channel)
                 )
              }
              val classifier = new RandomForest(attributes, features.toArray, labels.toArray, 25, 2, 2, 2, 1, DecisionTree.SplitRule.GINI) 
              println("CLASSIFIER VARIABLE IMPORTANCE:" + classifier.importance.mkString(" | ") )

              test.foreach{q =>
                    println(" -- -- TEST : " + q);

                     val feature   = Array[scala.Double](
                    getfracioninweek(q.date_time, q.date.ci),
                    getdateunit(q.date_time).toDouble,
                    getmonthunit(q.date_time).toDouble,
                    getNDays(q.date_time, q.date.ci).toDouble / 500.0,
                    getNDays(q.date.ci, q.date.co).toDouble / 10.0,
                    q.is_mobile.toDouble,
                    q.is_package.toDouble//,
//                    hotelChannelMap(q.channel)
                 )

                 var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                 classifier.predict(feature , posterior)
                 val prediction = posterior.toSeq.zipWithIndex.sortWith(_._1 > _._1).map(pair => (pair._1, hotelClustersOrdered(pair._2)) )
                 println("           Prediction --> "+ prediction.mkString("|") )
              }

           }
        }

*/


/////////////////////////////////////////////////////////////////////////////// 
///     B:  MAKE A PREDICTION BASED ON PAST HOTEL VISITED BY THE SAME USER 
///////////////////////////////////////////////////////////////////////////////  



        val resultsBMap = TEST
        .map(q => ( (q.user_id, q.srch_destination_id, q.hotel.country, q.hotel.market), q)  )
        .cogroup(trainDataRowRDD.map{q => ((q.user_id, q.srch_destination_id, q.hotel.country, q.hotel.market), q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrderedLeak = train.filter(q=>q.orig_destination_distance>=0).map{ q => (q.hotel_cluster, append_0(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list



              test.map{teq =>
                 var listA = Seq[(Int,Double)]()
                 var listB = Seq[(Int,Double)]()

                 val hotelClustersOrderedCityLeak = train.filter(q => q.user.city==teq.user.city && q.orig_destination_distance>=0).map{ q => (q.hotel_cluster, append_0(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                            .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list

                 val hotelClustersOrderedCityNoLeak = train.filter(q => q.user.city==teq.user.city && q.orig_destination_distance<0).map{ q => (q.hotel_cluster, append_0(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                            .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list


                 if(teq.orig_destination_distance<0){
                     listA = hotelClustersOrderedCityNoLeak.take(10)

                     if(listA.size>1 && train.count(q => true)>10){  //changed from 50
                        val hotelClustersMap     = listA.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters

                        val attributes = getFeatureAttributes() 
                        val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
                        val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
                        val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 500, 4)

                        val feature   = getFeature(teq)
                        var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                        classifier.predict(feature , posterior)
                        val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrderedCityNoLeak(a._2)._2 > b._1 * hotelClustersOrderedCityNoLeak(b._2)._2).map(pair => hotelClustersOrderedCityNoLeak(pair._2))
                        listA = (listA++prediction).distinct
                     }

                 }


                 if(hotelClustersOrderedLeak.size>0 && hotelClustersOrderedCityLeak.size<=0){
                    listB = hotelClustersOrderedLeak.take(10)

                     if(listB.size>1 && train.count(q => true)>10){  //changed from 50
                        val hotelClustersMap     = listB.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters

                        val attributes = getFeatureAttributes() 
                        val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
                        val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
                        val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 500, 4)

                        val feature   = getFeature(teq)
                        var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                        classifier.predict(feature , posterior)
                        val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrderedLeak(a._2)._2 > b._1 * hotelClustersOrderedLeak(b._2)._2).map(pair => hotelClustersOrderedLeak(pair._2))
                        listB = (listB++prediction).distinct
                     }


                 }

                 (teq.id, result(teq.hotel_cluster, (listA++listB).take(10)), null  )
           }
        }
              
        val resultsB = resultsBMap.map(a => (a._1, a._2) ).cache()
//        Map5(EvaluatePerf,  "User,Dest,HotelLoc", resultsB, Option(TESTsize))

/////////////////////////////////////////////////////////////////////////////// 
/// TESTB:  MAKE A PREDICTION BASED ON PAST HOTEL VISITED BY THE SAME USER  using MVA
///////////////////////////////////////////////////////////////////////////////  

        val resultsTestBMap = TEST
        .map(q => ( (q.user_id, q.srch_destination_id, q.hotel.country, q.hotel.market), q)  )
        .cogroup(trainDataRowRDD.map{q => ((q.user_id, q.srch_destination_id, q.hotel.country, q.hotel.market), q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrderedLeak = train.filter(q=>q.orig_destination_distance>=0).map{ q => (q.hotel_cluster, append_0(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list



              test.map{teq =>
                 var listA = Seq[(Int,Double)]()
                 var listB = Seq[(Int,Double)]()

                 val hotelClustersOrderedCityLeak = train.filter(q => q.user.city==teq.user.city && q.orig_destination_distance>=0).map{ q => (q.hotel_cluster, append_0(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                            .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list

                 val hotelClustersOrderedCityNoLeak = train.filter(q => q.user.city==teq.user.city && q.orig_destination_distance<0).map{ q => (q.hotel_cluster, append_0(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                            .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list


                 if(teq.orig_destination_distance<0){
                     listA = hotelClustersOrderedCityNoLeak.take(10)

                     if(listA.size>1 && train.count(q => true)>10){  //changed from 50
                        val hotelClustersMap     = listA.map((a=>a._1)).take(7).zipWithIndex.toMap  //only keep the 2 most likely clusters

                        val attributes = getFeatureAttributes() 
                        val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
                        val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
                        val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 250, 3)

                        val feature   = getFeature(teq)
                        var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                        classifier.predict(feature , posterior)
                        val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrderedCityNoLeak(a._2)._2 > b._1 * hotelClustersOrderedCityNoLeak(b._2)._2).map(pair => hotelClustersOrderedCityNoLeak(pair._2))
                        listA = (listA++prediction).distinct
                     }

                 }


                 if(hotelClustersOrderedLeak.size>0 && hotelClustersOrderedCityLeak.size<=0){
                    listB = hotelClustersOrderedLeak.take(10)

                     if(listB.size>1 && train.count(q => true)>10){  //changed from 50
                        val hotelClustersMap     = listB.map((a=>a._1)).take(7).zipWithIndex.toMap  //only keep the 2 most likely clusters

                        val attributes = getFeatureAttributes() 
                        val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
                        val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
                        val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 250, 3)

                        val feature   = getFeature(teq)
                        var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                        classifier.predict(feature , posterior)
                        val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrderedLeak(a._2)._2 > b._1 * hotelClustersOrderedLeak(b._2)._2).map(pair => hotelClustersOrderedLeak(pair._2))
                        listB = (listB++prediction).distinct
                     }


                 }

                 (teq.id, result(teq.hotel_cluster, (listA++listB).take(10)), null  )
           }
        }
        
        val resultsTESTB = resultsTestBMap.map(a => (a._1, a._2) ).cache()
//        Map5(EvaluatePerf,  "Destination Id, HotelLoc", resultsC, Option(TESTsize))




/////////////////////////////////////////////////////////////////////////////// 
///   ALS:  MAKE A PREDICTION BASED ON ALS 
///////////////////////////////////////////////////////////////////////////////  
/*
   val userIndexMapping = sc.broadcast( trainDataRowRDD.map{q => (q.srch_destination_id, q.hotel.country,q.hotel.market) }.distinct.zipWithUniqueId.collectAsMap )
   val ratings          = trainDataRowRDD.map{ q => 
      val INDEX:Int = userIndexMapping.value.getOrElse( (q.srch_destination_id, q.hotel.country,q.hotel.market) , 999999.toLong ).toInt
      Rating( INDEX , q.hotel_cluster, append_1(q) ) 
   }

   println("train ALS")

   val rank = 75
   val numIterations = 10
   val model = ALS.train(ratings, rank, numIterations, 0.01)
   

   println("ALS training is done")
   val testForALS = TEST.map{q =>
      val INDEX:Int = userIndexMapping.value.getOrElse( (q.srch_destination_id, q.hotel.country,q.hotel.market) , 999999.toLong ).toInt
      INDEX}
   .distinct
   .flatMap{ INDEX =>
      val hotelClusters = Array.fill[Int](100)(0).zipWithIndex.map(_._2)
      hotelClusters.toSeq.map( cluster => (INDEX, cluster) )
   }
   val ALSoutput = model.predict(testForALS).map( r => (r.user , Seq[(Int,java.lang.Double)]( (r.product, r.rating) ) ) ).reduceByKey(_++_)
   .mapValues{ a => a.sortWith{ case(a,b) =>  a._2 > b._2 }.take(7) } 

   val resultsALS = TEST.map{q => val INDEX:Int = userIndexMapping.value.getOrElse( (q.srch_destination_id, q.hotel.country,q.hotel.market) , 999999.toLong ).toInt; (INDEX, q)}
   .cogroup(ALSoutput)
   .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
   .flatMap{ in =>
      val key = in._1
      var test = in._2._1
      val lookup  = in._2._2.head

      test.map{q => 
              (q.id, result(q.hotel_cluster, lookup)) 
      } 
   }
   
   println("ALS applied to testing sample")
*/





/////////////////////////////////////////////////////////////////////////////// 
///     C:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION, COUNTRY, MARKET
///////////////////////////////////////////////////////////////////////////////  

        val resultsCMap = TEST
        .map(q => ( (q.srch_destination_id, q.hotel.country, q.hotel.market), q)  )
        .cogroup(trainDataRowRDD.map{q => ((q.srch_destination_id, q.hotel.country,q.hotel.market), q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters


           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list

           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(7).zipWithIndex.toMap  //only keep the 2 most likely clusters

            if(hotelClustersOrdered.size>1 && train.count(q => true)>10){  //changed from 50
              val attributes = getFeatureAttributes() 
              val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
              val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 250, 3)

              test.map{q =>
                     val feature   = getFeature(q)
                     var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                     classifier.predict(feature , posterior)
                   val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
        

        val resultsC = resultsCMap.map(a => (a._1, a._2) ).cache()
//        Map5(EvaluatePerf,  "Destination Id, HotelLoc", resultsC, Option(TESTsize))


/////////////////////////////////////////////////////////////////////////////// 
/// TEST3b:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION, COUNTRY, MARKET using an MVA
///////////////////////////////////////////////////////////////////////////////  

        val resultsTEST3bMap = TEST
        .map(q => ( (q.srch_destination_id, q.hotel.country, q.hotel.market), q)  )
        .cogroup(trainDataRowRDD.map{q => ((q.srch_destination_id, q.hotel.country,q.hotel.market), q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) =>
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters


           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list

           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(7).zipWithIndex.toMap  //only keep the 2 most likely clusters

            if(hotelClustersOrdered.size>1 && train.count(q => true)>10){  //changed from 50
              val attributes = getFeatureAttributes() 
              val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
              val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 250, 2)

              test.map{q =>
                     val feature   = getFeature(q)
                     var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                     classifier.predict(feature , posterior)
                   val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), classifier.importance)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
//        Future {
//        var Importance = resultsTEST3bMap.filter(a => a._3!=null).map(a => a._3).reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2)   )
//        println("VARIABLE IMPORTANCE :") 
//        Importance.foreach(a => println((a/Importance.sum).toString))
//        }

        val resultsTEST3b = resultsTEST3bMap.map(a => (a._1, a._2) ).cache()
//        Map5(EvaluatePerf,  "Destination Id, HotelLoc (with TEST3b)", resultsTEST3b, Option(TESTsize))


/////////////////////////////////////////////////////////////////////////////// 
///     D:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION
///////////////////////////////////////////////////////////////////////////////  

/*
        val hotelLocRDD = trainDataRowRDD                                                                     //WILL BE  PairRDD[DestId , Map[Marker , Top5clusters]  ]
        .map{q =>  ( q.srch_destination_id, q )                                                                                       //flat map to save two output entries per input entries, to work with destId/market/country and destId granularity
        }.groupByKey()
        .map{ case(key, querries) => 
            val hotelClustersOrdered = querries.map{ q => (q.hotel_cluster, append_1(q)) }
               .groupBy(_._1)
               .map{ pair => pair._2.reduce{(a,b) => (a._1, a._2 + b._2)} }
               .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) } }
               //.map((a=>a._1)) 
           (key, hotelClustersOrdered )
        }

        val resultsD = TEST
        .map(q => ( q.srch_destination_id, q)  )
        .cogroup(hotelLocRDD)
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val lookup = value._2 //matching hotel location mapping best clusters

           test.map{q => 
              (q.id, result(q.hotel_cluster, lookup.head.take(10) ))
           }
        }.cache()
        //Map5(EvaluatePerf,  "Destination Id, HotelLoc LowGran", resultsD, Option(TESTsize))
*/


/////////////////////////////////////////////////////////////////////////////// 
///     D:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION WITH MVA
///////////////////////////////////////////////////////////////////////////////  

        val resultsDMap = TEST
        .map(q => ( q.srch_destination_id, q)  )
        .cogroup(trainDataRowRDD.map{q => (q.srch_destination_id, q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list


           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters

            if(hotelClustersOrdered.size>1 && train.count(q => true)>10){  //changed from 50
              val attributes = getFeatureAttributes() 
              val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
              val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 500, 3)

              test.map{q =>
                     val feature   = getFeature(q)
                     var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                     classifier.predict(feature , posterior)
                   val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
        val resultsD = resultsDMap.map(a => (a._1, a._2) ).cache()
//        Map5(EvaluatePerf,  "Destination Id", resultsD, Option(TESTsize))


/////////////////////////////////////////////////////////////////////////////// 
/// TEST5 :  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION using an MVA
///////////////////////////////////////////////////////////////////////////////  
/*
        val resultsTEST5Map = TEST
        .map(q => ( q.srch_destination_id, q)  )
        .cogroup(trainDataRowRDD.map{q => (q.srch_destination_id, q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list


           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters

            if(hotelClustersOrdered.size>1 && train.count(q => true)>10){  //changed from 50
//              val attributes = Array[Attribute]( new NumericAttribute("Date"))//, new NumericAttribute("BookingPeriod"), new NumericAttribute("DaysBeforeCheckin") )  
              var labels   = ArrayBuffer[scala.Int]()
              var features = ArrayBuffer[Array[scala.Double] ]()
              train.foreach{ q =>
                 if(hotelClustersMap.contains(q.hotel_cluster)){ 
                    labels += hotelClustersMap(q.hotel_cluster)
                    features += getFeature(q) 
                 }
              }
              // (Attribute[] attributes, double[][] x, int[] y, int ntrees, int maxNodes, int nodeSize, int mtry, double subsample, DecisionTree.SplitRule rule) {
              val classifier = new AdaBoost(null,  features.toArray, labels.toArray, 250, 2)

              test.map{q =>
                     val feature   = getFeature(q)
                     var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                     classifier.predict(feature , posterior)
                   val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
        val resultsTEST5 = resultsTEST5Map.map(a => (a._1, a._2) ).cache()
        Map5(EvaluatePerf,  "Destination Id (with Test5)", resultsTEST5, Option(TESTsize))


*/


/////////////////////////////////////////////////////////////////////////////// 
///     E0:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION COUNTRY FOR SAME DESTINATION TYPE
///////////////////////////////////////////////////////////////////////////////  

        val hotelCountryDestTypeRDD = trainDataRowRDD                                                                  //WILL BE  PairRDD[Country , Top5clusters  ]
        .map(q => ( (q.hotel.country, q.srch_destination_type_id, q.hotel_cluster) , append_2(q))  )                                       //group by country/cluster
        .reduceByKey(_+_)                                                                                      //count per group
        .map( pair => {val key = pair._1; val value=pair._2; ((key._1, key._2), Seq( (key._3, value) ) ) })              //regroup by country/market
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group
        .mapValues( a => a.sortWith{  case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) } }
                   )//       .map(_._1)  )                                                  //collapste various clusters to a list
 
        //c2) make the prediction
        val resultsE0 = TEST
        .map(q => ( (q.hotel.country, q.srch_destination_type_id), q)  )
        .cogroup(hotelCountryDestTypeRDD)
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val lookup = value._2 //matching hotel location mapping best clusters

           test.map{q =>  
                 (q.id, result(q.hotel_cluster, lookup.head.take(10) ))
          }
        }.cache()
//        Map5(EvaluatePerf,  "Hotel Country", resultsE0, Option(TESTsize))


/////////////////////////////////////////////////////////////////////////////// 
/// TEST6 :  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION COUNTRY FOR SAME DESTINATION TYPE WITH MVA
///////////////////////////////////////////////////////////////////////////////  


        val resultsTEST6Map = TEST
        .map(q => ( (q.hotel.country, q.srch_destination_type_id), q)  )
        .cogroup(trainDataRowRDD.map{q => ((q.hotel.country, q.srch_destination_type_id), q)} )    //That's a huge shuffle !!WARNING to reduce the time repartition to 1k
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val train = value._2 //matching hotel location mapping best clusters

           val hotelClustersOrdered = train.map{ q => (q.hotel_cluster, append_1(q)) }.groupBy(_._1).map{ case(key,listKeyValuePairs) => (key, listKeyValuePairs.map(_._2).reduce(_+_))   }  //get Map of cluster -> counts
                                   .toSeq.sortWith{ case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) }  }                //order the list
           val hotelClustersMap     = hotelClustersOrdered.map((a=>a._1)).take(5).zipWithIndex.toMap  //only keep the 2 most likely clusters

            if(hotelClustersOrdered.size>1 && train.count(q => true)>10){  //changed from 50
              val attributes = getFeatureAttributes() 
              val labels:Array[scala.Int] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q=> hotelClustersMap(q.hotel_cluster).toInt ).toArray
              val features:Array[Array[scala.Double] ] = train.filter(q=> hotelClustersMap.contains(q.hotel_cluster)).map(q => getFeature(q) ).toArray
              val classifier = new AdaBoost(attributes,  features.toArray, labels.toArray, 250, 2)

              test.map{q =>
                     val feature   = getFeature(q)
                     var posterior = Array.fill[scala.Double]( hotelClustersMap.size)(0.0)
                     classifier.predict(feature , posterior)
                   val prediction = posterior.toSeq.zipWithIndex.sortWith( (a,b) => a._1 * hotelClustersOrdered(a._2)._2 > b._1 * hotelClustersOrdered(b._2)._2).map(pair => hotelClustersOrdered(pair._2))
                     (q.id, result(q.hotel_cluster, (prediction ++ hotelClustersOrdered).distinct.take(10)), null)
              }
           }else{
              test.map{q => (q.id, result(q.hotel_cluster, hotelClustersOrdered.take(10)), null  ) }
           }
        }
        val resultsTEST6 = resultsTEST6Map.map(a => (a._1, a._2) ).cache()
//        Map5(EvaluatePerf,  "Hotel Country (with Test6)", resultsTEST6, Option(TESTsize))





/////////////////////////////////////////////////////////////////////////////// 
///     E:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION COUNTRY 
///////////////////////////////////////////////////////////////////////////////  

        val hotelCountryRDD = trainDataRowRDD                                                                  //WILL BE  PairRDD[Country , Top5clusters  ]
        .map(q => ( (q.hotel.market, q.hotel_cluster) , append_2(q))  )                                       //group by country/cluster
        .reduceByKey(_+_)                                                                                      //count per group
        .map( pair => {val key = pair._1; val value=pair._2; (key._1, Seq( (key._2, value) ) ) })              //regroup by country/market
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group
        .mapValues( a => a.sortWith{  case(a,b) =>  if( a._2 != b._2  ){a._2 > b._2}else{ clusterFrequencyBD.value(a._1) > clusterFrequencyBD.value(b._1) } }
                   )//       .map(_._1)  )                                                  //collapste various clusters to a list
 
        //c2) make the prediction
        val resultsE = TEST
        .map(q => ( q.hotel.market, q)  )
        .cogroup(hotelCountryRDD)
        .filter{ case(key,value) => ((value._1.size>0) && (value._2.size>0)) }
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val lookup = value._2 //matching hotel location mapping best clusters

           test.map{q =>  
                 (q.id, result(q.hotel_cluster, lookup.head.take(10) ))
          }
        }.cache()
        //Map5(EvaluatePerf,  "Hotel Country", resultsE, Option(TESTsize))

/////////////////////////////////////////////////////////////////////////////// 
///     F:  MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER WORDLWIDE 
///////////////////////////////////////////////////////////////////////////////  

        val Top10HotelsWorldwide = clusterFrequency.take(10).toSeq//     .map(_._1).toSeq
        val resultsF = TEST
        .map(q => (q.id, result(q.hotel_cluster, Top10HotelsWorldwide)) ).cache()
        //Map5(EvaluatePerf,  "Cluster Frequencies", resultsF, Option(TESTsize))

   val ListWithId = TEST.map{ q => (q.id, result(q.hotel_cluster, Seq[(Int,Double)]() )) }.cache()


//   preprocess(Array(ListWithId, resultsA, resultsTEST0, resultsB, resultsC, resultsD, resultsE0, resultsE, resultsF, resultsTEST3, resultsTEST4 ) )

//   only for debugging purposes
//   groupAndSave(EvaluatePerf, true, "groupEmpty", Array(ListWithId) )
//   groupAndSave(EvaluatePerf, true, "groupA", Array(ListWithId, resultsA) )
//   groupAndSave(EvaluatePerf, true, "groupAB", Array(ListWithId, resultsA, resultsB) )
//   groupAndSave(EvaluatePerf, true, "groupABC", Array(ListWithId, resultsA, resultsB, resultsC) )
//   groupAndSave(EvaluatePerf, true, "groupABCD", Array(ListWithId, resultsA, resultsB, resultsC, resultsD) )

//   groupAndSave(EvaluatePerf, false, "TEST1", Array(ListWithId, resultsTEST1, resultsB, resultsC, resultsD, resultsE, resultsF ) )
//   groupAndSave(EvaluatePerf, false, "TEST1andA", Array(ListWithId, resultsTEST1, resultsA, resultsB, resultsC, resultsD, resultsE, resultsF ) )
//   groupAndSave(EvaluatePerf, false, "TEST1Cleaned", Array(ListWithId, resultsTEST1Cleaned, resultsB, resultsC, resultsD, resultsE, resultsF ) )

//        Map5(EvaluatePerf,  "Destination Id, HotelLoc (with NB)", resultsTEST5, Option(TESTsize))
//   groupAndSave(EvaluatePerf, true, "groupAllNB", Array(ListWithId, resultsA, resultsB, resultsTEST5, resultsD, resultsE, resultsF ) )

   Array[Future[Unit]](
      Future{ groupAndSave(EvaluatePerf, true, "groupAll", Array(ListWithId, resultsA, resultsB, resultsC, resultsD, resultsE0, resultsE, resultsF ) ) }
//      Future{ groupAndSave(EvaluatePerf, true, "groupTB", Array(ListWithId, resultsA, resultsTESTB, resultsC, resultsD, resultsE0, resultsE, resultsF ) ) }

//      Future{ groupAndSave(EvaluatePerf, true, "groupALS", Array(ListWithId, resultsA, resultsB, resultsALS, resultsC, resultsD, resultsE0, resultsE, resultsF ) ) }



//      Future{ groupAndSave(EvaluatePerf, true, "groupAllT0b", Array(ListWithId, resultsTEST0b, resultsB, resultsC, resultsD, resultsE0, resultsE, resultsF ) ) },
//      Future{ groupAndSave(EvaluatePerf, true, "groupAllT3b", Array(ListWithId, resultsA, resultsB, resultsTEST3b, resultsD, resultsE0, resultsE, resultsF ) ) }
////      Future{ groupAndSave(EvaluatePerf, true, "groupAllT5", Array(ListWithId, resultsA, resultsB, resultsC, resultsTEST5, resultsE0, resultsE, resultsF ) ) },
////      Future{ groupAndSave(EvaluatePerf, true, "groupAllT6", Array(ListWithId, resultsA, resultsB, resultsC, resultsD, resultsTEST6, resultsE, resultsF ) ) }

   ).foreach{f => Await.ready(f, 10000 minutes) }




//   groupAndSave(EvaluatePerf, true, "groupAll", Array(ListWithId, resultsTEST1, resultsB, resultsC, resultsD, resultsE, resultsF ) )



    val totalelapsedTime = (System.nanoTime() - totalstartTime) / 1e9
    System.err.println(s"TOTAL time: $totalelapsedTime seconds")
    sc.stop()
    System.err.println("sparkContext stopped")
    System.exit(0)
  }
}


