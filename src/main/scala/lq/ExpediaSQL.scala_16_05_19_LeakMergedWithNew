package lq

import java.lang._ 
import scala.reflect.runtime.universe.TypeTag
import scala.reflect.runtime.universe.TypeTag._
import scala.collection.mutable.ArrayBuffer
import scala.concurrent.duration._
import scala.concurrent._

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.expressions.Window
//import org.apache.spark.sql.hive.HiveContext

import org.apache.spark.ml.classification.{OneVsRest, LogisticRegression}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.clustering._
import org.apache.spark.mllib.random._

import org.apache.spark.ml._
import org.apache.spark.ml.classification._
import org.apache.spark.ml.feature._
import org.apache.spark.ml.evaluation._

import java.lang.Math
import java.text.SimpleDateFormat
import java.util.concurrent.TimeUnit
import java.util.{Calendar, Date}

//import smile.classification._
//import smile.data.{Attribute, NumericAttribute, NominalAttribute};

object ExpediaSQL {

   def getyear(s:String):Int = {
      val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      var date = formatter.parse(s);
      val calendar = Calendar.getInstance();
      calendar.setTime(date);
      calendar.get(Calendar.YEAR) % 100;
   }

   def getmonth(s:String):Int = {
      val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      var date = formatter.parse(s);
      val calendar = Calendar.getInstance();
      calendar.setTime(date);
      calendar.get(Calendar.MONTH );
   }

   def getmonthunit(s:String):Double = {
      val formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
      var date = formatter.parse(s);
      val calendar = Calendar.getInstance();
      calendar.setTime(date);
      calendar.get(Calendar.DAY_OF_YEAR) / 365;
   }

   def getNDays(ci:String, co:String):Int = {
      try{
         val formatter = new SimpleDateFormat("yyyy-MM-dd");
         var dateCI = formatter.parse(ci);
         var dateCO = formatter.parse(co);
         return TimeUnit.DAYS.convert(dateCO.getTime()-dateCI.getTime(), TimeUnit.MILLISECONDS).toInt
      }catch{
         case unknown : Throwable => return -99
      }
   }

  case class user_location(country:Int , region:Int , city:Int)
  case class hotel_location(continent:Int , country:Int , market:Int);
  case class cico_date(ci:String , co:String)
  case class querry(id:Long, date_time:String , site_name:Int , posa_continent:Int , user:user_location, orig_destination_distance:Int , user_id:Int , is_mobile:Int , is_package:Int , channel:Int , date:cico_date, srch_adults_cnt:Int , srch_children_cnt:Int , srch_rm_cnt:Int , srch_destination_id:Int , srch_destination_type_id:Int , is_booking:Int , cnt:Int , hotel:hotel_location, hotel_cluster:Int)
  case class result(q:querry, p:Seq[Int])

  def Map5(EvaluatePerf:Boolean, label:String, results:org.apache.spark.rdd.RDD[result]):org.apache.spark.rdd.RDD[result] = {
       if(EvaluatePerf){
          val out = results.map( r => {
             Array[scala.Double](1 , r.p.view.zipWithIndex.map( p => if(p._1==r.q.hotel_cluster){1.toDouble/(p._2+1.0)}else{0.0} ).foldLeft(0.toDouble)(Math.max) )
          }).reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2) )
          println("Performance for %30s with MAP@5 = %8f/%8f = %8.4f".format(label, out(1), out(0), out(1)/out(0)) )
       }
       results
  }

  def append_0(q:querry):Double = { ((getyear(q.date_time) - 12)*12 + (getmonth(q.date_time) - 12)) }
  def append_1(q:querry):Double = { append_0(q) * append_0(q) * (3 + 17.60*q.is_booking) }
  def append_2(q:querry):Double = { 3 + 5.56*q.is_booking }

  def main(args: Array[String]) {
        val conf = new SparkConf()
        conf.setAppName("ExpediaSQL")
        conf.set("spark.driver.memory", "4g")
        conf.set("spark.executor.memory", "4g")
        val sc = new SparkContext(conf)
        sc.setLogLevel("WARN")

        val sqlContext = new org.apache.spark.sql.SQLContext(sc)
        import sqlContext.implicits._


        //OPTIONS
        val EvaluatePerf = false;

 	//Loading the train data into RDD with split
	val inputTrainDataRowRDD =sc.textFile("file:///afs/cern.ch/user/q/querten/workspace/public/SparkTest/Data/train.csv", 300).filter(! _.contains("date_time") ).map(_.split(",")).zipWithIndex().map{ case(q,index) => 
           var distance:Int = {if(q(6).length>0){(q(6).toDouble*1E5).toInt}else{-1} }
           querry(-1*index, q(0),q(1).toInt,q(2).toInt, user_location(q(3).toInt,q(4).toInt,q(5).toInt),distance,q(7).toInt,q(8).toInt,q(9).toInt,q(10).toInt, cico_date(q(11),q(12)),
                  q(13).toInt, q(14).toInt, q(15).toInt, q(16).toInt, q(17).toInt, q(18).toInt, q(19).toInt, hotel_location(q(20).toInt, q(21).toInt, q(22).toInt), q(23).toInt )
        }

 	//Loading the test data into RDD with split
	val inputTestDataRowRDD =sc.textFile("file:///afs/cern.ch/user/q/querten/scratch0/test.csv").filter(! _.contains("date_time") ).map(_.split(",")).map{ q =>
           var distance:Int = {if(q(7).length>0){(q(7).toDouble*1E5).toInt}else{-1} }
           querry(q(0).toLong, q(1),q(2).toInt,q(3).toInt,user_location(q(4).toInt,q(5).toInt,q(6).toInt),distance,q(8).toInt,q(9).toInt,q(10).toInt,q(11).toInt,cico_date(q(12),q(13)), 
                  q(14).toInt, q(15).toInt, q(16).toInt, q(17).toInt, q(18).toInt, 1, 1, hotel_location(q(19).toInt, q(20).toInt, q(21).toInt), -1 )
        }
        
        //define the training and test samples that we are going to use
        val Array(trainDataRowRDD, testDataRowRDD) = {
           if(EvaluatePerf){
              Array(inputTrainDataRowRDD.filter(q => getyear(q.date_time)<14 || getmonthunit(q.date_time)<0.5), inputTrainDataRowRDD.filter(q => getyear(q.date_time)==14 && getmonthunit(q.date_time)>0.5 && q.is_booking==1 && q.cnt==1 ) )
           }else{
              Array(inputTrainDataRowRDD, inputTestDataRowRDD )
           }
        }.map(_.cache())


        //Init : DEFINE THE OUTPUT FORMAT AND USE IT EVERYWHERE  (ALLOWS to comment some part of the code without hurts)
        val totalstartTime = System.nanoTime()
        var results = testDataRowRDD.map(q => result(q, Seq[Int]()) );

/////////////////////////////////////////////////////////////////////////////// LEAK

        //Step a) : MAKE PREDICTION BASED ON DISTANCE DATA LEAK
        //a1) make a map of top clusters and Classifier for each leak entry
        val groupedLeakClassifier = trainDataRowRDD
        .filter(q => q.orig_destination_distance >=0 )
        .map(q => ( (q.user.city, q.orig_destination_distance) , q)  )
        .groupByKey().mapValues( Querries => {
           val hotelClustersOrdered = Querries.map(q => {  (q.hotel_cluster, 1)})//append_0(q)) })
                                      .groupBy(_._1)
                                      .map({ pair => pair._2.reduce{(a,b) => (a._1, a._2 + b._2)} })
                                      .toSeq.sortWith(_._2 > _._2) 
                                      .map((a=>a._1))
           val hotelClustersMap     = hotelClustersOrdered.zipWithIndex.toMap
           val classifier = null
           (hotelClustersOrdered.toSeq, classifier)
        })

        //Step a2)  use the previously created map to make the prediction
        results = results.filter(r=>r.q.orig_destination_distance<0) ++ results.filter(r=>r.q.orig_destination_distance>=0)
        .map(r => ( (r.q.user.city, r.q.orig_destination_distance ), r)  )
        .cogroup(groupedLeakClassifier)
        .flatMap( in => {
           val key = in._1
           var test = in._2._1
           val leak = in._2._2

           if(key._2>=0 && leak.size>0){                                                                       //make sure we have a leak AND the distance is >0 
              val leakClusters   = leak.head._1
              val leakClassifier = leak.head._2
                 test.map(r => result(r.q, r.p ++ leakClusters) ) 
           }else{ 
              test 
           }
        })

///////////////////////////////////////////////////////////////////////////////


/*
        //Step a Bis : MAKE A PREDICTION BASED ON PAST HOTEL VISITED BY THE SAME USER when there is no orig_destination_distance
        val pasUserDebugD = trainDataRowRDD
        .filter(q => q.is_booking==1 )  //do this only if we don't have distance match and we have a booking
        .flatMap(q => { 
           Seq( ( ({if(q.orig_destination_distance<0){0}else{1}}, q.user_id, q.user.city, q.srch_destination_id, q.hotel.country, q.hotel.market, q.hotel_cluster) , append_0(q) ) 
              , ( ({if(q.orig_destination_distance<0){0}else{1}}, q.user_id, -1         , q.srch_destination_id, q.hotel.country, q.hotel.market, q.hotel_cluster) , append_0(q) ) )
        })
        .reduceByKey(_+_)                                                                                      //count per cluster
        .map( case (key, value) => ( (key._1, key._2, key._3, key._4, key._5, key._6), Seq( (key._7, value) ) ) )              //regroup by key / (cluster/value)
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group
        .mapValues( a => a.sortWith(_._2 > _._2).map(_._1)  )                                          //collapste various clusters to a list
        .map( case (key, value) => ( (key._2, key._4, key._5, key._6), Map( ( (key._1, key._3) , value) )  ) ) 
        .reduceByKey((a,b) => (a++b) )

        println("debugD count = " + pasUserDebugD.count)
        pasUserDebugD.take(20).foreach(println)


results = Map5(EvaluatePerf,  "debugD",
        //Step aTris 2)  use the previously created map to make the prediction
        results// = results.filter(r=>r.q.orig_destination_distance>0) ++ results.filter(r=>r.q.orig_destination_distance<0)
        .map(r => ( (r.q.user_id, r.q.srch_destination_id, r.q.hotel.country, r.q.hotel.market), r)  )
        .cogroup(pasUserDebugD)
//        .filter( in => (in._2._1.size>0 && in._2._2.size>0 ))
        .flatMap( in => {
           val key = in._1
           var test = in._2._1
           val map  = in._2._2

           if(map.size>0 ){                                                                       //make sure we have a matching entry in the map 
              test.map(r => {
                 var prediction = Seq[Int]()
                 if(r.q.orig_destination_distance>=0){
                    if(map.head.contains((1, -1)) && !map.head.contains((1, r.q.user.city))) prediction = prediction ++ map.head.getOrElse((1,-1), Seq[Int]())
                 }else{
                     prediction = prediction ++ map.head.getOrElse((0, r.q.user.city), Seq[Int]())
                 }
                 result(r.q, r.p ++ prediction ) 
              }) 
           }else{ 
              test 
           }
        })
)
*/
/*
        //Step b) : MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION
        //b1- get the most probable cluster per destinationId and country/market at once.  
        val hotelLocRDD = trainDataRowRDD                                                                     //WILL BE  PairRDD[DestId , Map[Marker , Top5clusters]  ]
        .flatMap(q => {                                                                                       //flat map to save two output entries per input entries, to work with destId/market/country and destId granularity
           val weight = {if(q.is_booking==1)1.0 else 0.15}
           var toReturn = Seq( ((q.srch_destination_id, (-1,-1)                         , q.hotel_cluster),  weight) )
           if(getyear(q.date_time)==14){
               toReturn = Seq( ((q.srch_destination_id, (q.hotel.country,q.hotel.market), q.hotel_cluster),  weight) ) ++ toReturn
           }
           toReturn
        })                       
        .reduceByKey(_+_)                                                                                      //count per group
        .map( pair => {val key = pair._1; val value=pair._2; ((key._1, key._2) , Seq( (key._3, value) ) ) })   //regroup by DestId/market
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group
        .mapValues( a => a.sortWith(_._2 > _._2).map(_._1)  )                                          //collapste various clusters to a list
        .map( pair => {val key = pair._1; val value=pair._2; ( key._1, Map(key._2 -> (value, null)) ) })               //regroup by DestId  //debugging modify value to (value, null)
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group
*/
        //println("Destination Id with more than 1 channel = " + hotelLocRDD1.filter( a => a._2.size>1).count() )
/*
        //b2) make the prediction
        results = results
        .map(r => ( r.q.srch_destination_id, r)  )
        .cogroup(hotelLocRDD)
        .flatMap( in => {
           val key = in._1     //hotel location
           var test = in._2._1 //matching querry data
           val lookup = in._2._2 //matching hotel location mapping best clusters

           if(lookup.size>0){  //make sure this destId is in the lookup table
              test.map(r => {
                 var prediction = lookup.head.getOrElse((r.q.hotel.country,r.q.hotel.market), Seq[Int]())
                 if(prediction.size<5){ prediction= prediction ++ lookup.head.getOrElse((-1,-1), Seq[Int]()) }
                 result(r.q, (r.p ++ prediction).distinct ) 
              }) 
           }else{ 
              test 
           }
        })
*/


        //Step b) : MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION
        //b1- get the most probable cluster per destinationId and country/market at once.  
        val hotelLocRDD = trainDataRowRDD                                                                     //WILL BE  PairRDD[DestId , Map[Marker , Top5clusters]  ]
        .flatMap(q => {                                                                                       //flat map to save two output entries per input entries, to work with destId/market/country and destId granularity
           val weight = {if(q.is_booking==1)1.0 else 0.15}
           var toReturn = Seq( ((q.srch_destination_id, (-1,-1)                         ), (q.hotel_cluster,  weight) ) )
           if(getyear(q.date_time)==14){
               toReturn = Seq( ((q.srch_destination_id, (q.hotel.country,q.hotel.market)), (q.hotel_cluster,  weight) ) ) ++ toReturn
           }
           toReturn
        }).groupByKey().mapValues( pairs => pairs.map{ case(cluster, w) => (cluster,w) }.groupBy(_._1).map({ pair => pair._2.reduce{(a,b) => (a._1, a._2 + b._2)} }).toSeq.sortWith(_._2 > _._2).map((a=>a._1)) )
        .map({ case(key, value) => (key._1, Map(key._2 -> (value, null)) ) })               //regroup by DestId  //debugging modify value to (value, null)
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group


/*
        //Step b) : MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION
        //b1- get the most probable cluster per destinationId and country/market at once.  
        val hotelLocRDD = trainDataRowRDD                                                                     //WILL BE  PairRDD[DestId , Map[Marker , Top5clusters]  ]
        .flatMap(q => {                                                                                       //flat map to save two output entries per input entries, to work with destId/market/country and destId granularity
           var toReturn = Seq( ((q.srch_destination_id, (q.hotel.country,q.hotel.market), q.hotel_cluster),  q) ) 
               toReturn = Seq( ((q.srch_destination_id, (-1,-1)                         , q.hotel_cluster),  q) ) ++ toReturn
           toReturn
        })                      
        .groupByKey().map( pair => {
           val key = pair._1
           val Querries = pair._2
           val hotelClustersOrdered = Querries.map{q =>
              var weight = {if(q.is_booking==1)1.0 else 0.15}
              if(getyear(q.date_time)<14 && key._1>=0)weight=0.0
//              val weight = {if(key._1<0){append_2(q)}else{append_1(q)}}
              (q.hotel_cluster, if(key._1<0){append_2(q)}else{append_1(q)} ) 
           }.groupBy(_._1).map({ pair => pair._2.reduce{(a,b) => (a._1, a._2 + b._2)} }).toSeq.sortWith(_._2 > _._2).map(a=>a._1)
           val hotelClustersMap     = hotelClustersOrdered.zipWithIndex.toMap

           val classifier = null

//           val attributes = Array[Attribute]( new NumericAttribute("BookingYear"), new NumericAttribute("BookingMonth"), new NumericAttribute("ndays"), new NumericAttribute("NRoom"), new NumericAttribute("KidFrac"), new NominalAttribute("Channel"), new NominalAttribute("is_package")  )  
//           var labels   = ArrayBuffer[scala.Int]()
//           var features = ArrayBuffer[Array[scala.Double] ]()

//           val classifier = {
//              if(hotelClustersOrdered.size>1){
//                 Querries.foreach( q => {
//                    labels += hotelClustersMap(q.hotel_cluster)
//                    features += Array( getyear(q.date_time).toDouble, getmonth(q.date_time).toDouble, getNDays(q.date.ci, q.date.co).toDouble, q.srch_rm_cnt.toDouble,   (q.srch_children_cnt.toDouble/(q.srch_children_cnt+q.srch_adults_cnt).toDouble), q.channel.toDouble, q.is_package.toDouble  )
//                 })
//                 new RandomForest(attributes, features.toArray, labels.toArray, 50, 2, 2, 2, 1, DecisionTree.SplitRule.GINI)
//               }else{
//                    null
//               }
//           }

           (key, (hotelClustersOrdered.toSeq, classifier))
        })
        .map({ case(key, value) => (key._1, Map(key._2 -> value) ) })               //regroup by DestId
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group


        //println("Destination Id with more than 1 channel = " + hotelLocRDD1.filter( a => a._2.size>1).count() )
*/
        //b2) make the prediction
        results = results
        .map(r => ( r.q.srch_destination_id, r)  )
        .cogroup(hotelLocRDD)
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val lookup = value._2 //matching hotel location mapping best clusters

           if(lookup.size>0){  //make sure this destId is in the lookup table
              test.map(r => {
                 var prediction = Seq[Int]()
//                 val feature   = Array( getyear(r.q.date_time).toDouble, getmonth(r.q.date_time).toDouble, getNDays(r.q.date.ci, r.q.date.co).toDouble, r.q.srch_rm_cnt.toDouble, (r.q.srch_children_cnt.toDouble/(r.q.srch_children_cnt+r.q.srch_adults_cnt).toDouble), r.q.channel.toDouble, r.q.is_package  )

                 val highGranularity = lookup.head.getOrElse((r.q.hotel.country,r.q.hotel.market), (Seq[Int](),null))
//                 if (highGranularity._2==null){
                     prediction = prediction ++ highGranularity._1
//                 }else{
//                     var posterior = Array.fill[scala.Double]( highGranularity._1.size)(0.0)
//                     highGranularity._2.predict(feature , posterior)
//                     prediction = prediction ++ posterior.toSeq.zipWithIndex.sortWith(_._1 > _._1).map(pair => highGranularity._1(pair._2))
//                 }

                 if(prediction.size<5){
                 val lowGranularity = lookup.head.getOrElse((-1, -1), (Seq[Int](),null))
//                 if (lowGranularity._2==null){
                     prediction = prediction ++ lowGranularity._1
//                 }else{
//                     var posterior = Array.fill[scala.Double]( lowGranularity._1.size)(0.0)
//                     lowGranularity._2.predict(feature , posterior)
//                     prediction = prediction ++ posterior.toSeq.zipWithIndex.sortWith(_._1 > _._1).map(pair => lowGranularity._1(pair._2))
//                 }
                 }
                 result(r.q, (r.p ++ prediction).distinct.take(5) )
              })
           }else{ 
              test 
           }
        }



        //Step c) : MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER AT DESTINATION COUNTRY
        //c1) get the most probable cluster for this country
        val hotelCountryRDD = trainDataRowRDD                                                                  //WILL BE  PairRDD[Country , Top5clusters  ]
        .map(q => ( (q.hotel.country, q.hotel_cluster) , {if(q.is_booking==1)1.0 else 0.167})  ) // append_0(q))  )                                       //group by country/cluster
        .reduceByKey(_+_)                                                                                      //count per group
        .map( pair => {val key = pair._1; val value=pair._2; (key._1, Seq( (key._2, value) ) ) })              //regroup by country/market
        .reduceByKey((a,b) => a++b )                                                                           //sum-up the list for each group
        .mapValues( a => a.sortWith(_._2 > _._2).map(_._1)  )                                                  //collapste various clusters to a list
 
        //c2) make the prediction
        results = results
        .map(r => ( r.q.hotel.country, r)  )
        .cogroup(hotelCountryRDD)
        .flatMap{ case(key, value) => 
           var test = value._1 //matching querry data
           val lookup = value._2 //matching hotel location mapping best clusters

           if(lookup.size>0){  //make sure this country is in the lookup table
              test.map(r => result(r.q, (r.p ++ lookup.head).distinct.take(5) ) ) 
           }else{ 
              test 
           }
        }



        //Step d) : MAKE PREDICTION BASED ON MOST FREQUENT CLUSTER WHERE EVER
        //d1) get the most probable cluster for this country
        val mostFrequentClusters = trainDataRowRDD                                                          //WILL BE  PairRDD[cluster]
        .map(q => ( q.hotel_cluster, 1 ) ) //append_0(q) )  )                                   //group by cluster
        .reduceByKey(_+_)                                                                                      //count per cluster
        .sortBy(_._2, false)                                                                                   //order the list (descending)
        .take(10).map(_._1).toSeq                                                                                    //keep the 5 most clusters
 
        //d2) make the prediction
        results = results.filter(r=>r.p.size>=5)++results.filter(r=>r.p.size<5)
        .map(r => result(r.q, (r.p ++ mostFrequentClusters).distinct.take(5) ) ) 



/*
    //some statistics
    val stat = results.map(r => Array[Boolean]( r.p.size<=0,  r.p.size==1,  r.p.size==2,  r.p.size==3,  r.p.size==4,  r.p.size==5, r.p.size>5) )
                      .map(r => r.map( b => if(b){1}else{0}) )
                      .reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2) )

    println("total test sample = " + stat.sum )
    println("Test Id with less than 5 predictions = " + (stat.sum - stat(5) - stat(6)) )
    println("Test Id with           0 predictions = " + stat(0) )
    println("Test Id with           1 predictions = " + stat(1) )
    println("Test Id with           2 predictions = " + stat(2) )
    println("Test Id with           3 predictions = " + stat(3) )
    println("Test Id with           4 predictions = " + stat(4) )
    println("Test Id with           5 predictions = " + stat(5) )
    println("Test Id with         > 5 predictions = " + stat(6) )
*/

    //Step FINAL): evaluate performance using map(5) function OR simply save the results for submission
    Map5(EvaluatePerf, "final", results)

    if(!EvaluatePerf){
       val submission = results
       .map( r => r.q.id.toString + "," + r.p.mkString(" ") )
       .coalesce(10)
       .saveAsTextFile("file:///afs/cern.ch/user/q/querten/scratch0/Expedia_16_05_14");
    }

    val totalelapsedTime = (System.nanoTime() - totalstartTime) / 1e9
    System.err.println(s"TOTAL time: $totalelapsedTime seconds")
  }
}


/*
         val x = Array(Array[scala.Double](1.0,0.0),Array[scala.Double](1.0, 0.0), Array[scala.Double](0.0,1.0), Array[scala.Double](0.0,1.0), Array[scala.Double](1.0,1.0))
         val y = Array[scala.Int](0, 0, 1, 1, 2)
         val Tree = new RandomForest(null, x, y, 10, 1, 1, 2, 1, DecisionTree.SplitRule.GINI)
         println("Tree predicts : " + Tree.predict(Array[scala.Double](1.0,0.0) ) )
         println("Tree predicts : " + Tree.predict(Array[scala.Double](0.0,1.0) ) )
         println("Tree predicts : " + Tree.predict(Array[scala.Double](1.0,1.0) ) )
         println("Tree predicts : " + Tree.predict(Array[scala.Double](0.0,0.0) ) )
         println("Tree predicts : " + Tree.predict(Array[scala.Double](0.5,0.5) ) )
         println("Tree predicts : " + Tree.predict(Array[scala.Double](2.0,2.0) ) )
         var prob = Array[scala.Double](0.0, 0.0, 0.0)
         println("Tree predicts : " + Tree.predict(Array[scala.Double](1.0,0.4) , prob) )
         println("probabilities = " + prob.mkString(" | "))
*/




/*


   //////////////////// PRELIMINARY WORK FOR MACHINE LEARNING STRATEGY
    val statDestId =  trainDataRowRDD
        .filter( q => getyear(q.date_time)==14)
        .map(q => ( (q.srch_destination_id, q.hotel.country,q.hotel.market), Seq( q.hotel_cluster))  )
        .reduceByKey((a,b) => (a++b).distinct )
        .map(pair => Array[Boolean]( pair._2.size<=0,  pair._2.size==1,  pair._2.size==2,  pair._2.size==3,  pair._2.size==4,  pair._2.size==5, pair._2.size>5) )
        .map(r => r.map( b => if(b){1}else{0}) )
        .reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2) )

    println("DestId total = " + statDestId.sum )
    println("DestId with less than 5 Clusters = " + (statDestId.sum - statDestId(5) - statDestId(6)) )
    println("DestId with           0 Clusters = " + statDestId(0) )
    println("DestId with           1 Clusters = " + statDestId(1) )
    println("DestId with           2 Clusters = " + statDestId(2) )
    println("DestId with           3 Clusters = " + statDestId(3) )
    println("DestId with           4 Clusters = " + statDestId(4) )
    println("DestId with           5 Clusters = " + statDestId(5) )
    println("DestId with         > 5 Clusters = " + statDestId(6) )


   //////////////////// PRELIMINARY WORK FOR BETTER LEAK EXPLOITATION

        val groupedLeakData = trainDataRowRDD
        .filter(q => q.orig_destination_distance >=0 )
        .map(q => ( (q.user.city, q.orig_destination_distance, getyear(q.date_time), q.hotel_cluster) , Seq(q))  )
        .reduceByKey(_++_)
        .map( case (key, value) => ((key._1, key._2, key._3) , Seq( (key._4, value) ) ) )
        .reduceByKey((a,b) => a++b )
        .mapValues( a => a.sortWith(_._2.size > _._2.size)  )

        val statLeak = groupedLeakData
        .map( case(key,value) => Array[Boolean]( value.size<=0,  value.size==1,  value.size==2,  value.size==3,  value.size==4,  value.size==5, value.size>5) )
        .map(r => r.map( b => if(b){1}else{0}) )
        .reduce( (a,b) => a.zip(b).map( pair => pair._1+pair._2) )

          println("Leak total = " + statLeak.sum )
          println("Leak with less than 5 Clusters = " + (statLeak.sum - statLeak(5) - statLeak(6)) )
          println("Leak with           0 Clusters = " + statLeak(0) )
          println("Leak with           1 Clusters = " + statLeak(1) )
          println("Leak with           2 Clusters = " + statLeak(2) )
          println("Leak with           3 Clusters = " + statLeak(3) )
          println("Leak with           4 Clusters = " + statLeak(4) )
          println("Leak with           5 Clusters = " + statLeak(5) )
          println("Leak with         > 5 Clusters = " + statLeak(6) )


        groupedLeakData
        .filter(a => a._2.size>1)
        .take(25).foreach(pair => {println("### " + pair._1); pair._2.foreach(pair => { println("  ### HotelCluster = " + pair._1); pair._2.foreach(println); } )} )


    /////////////////////////////////////////////////////////////////
*/


